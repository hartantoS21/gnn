{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install dgl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-29T04:56:11.805014Z","iopub.execute_input":"2023-11-29T04:56:11.805582Z","iopub.status.idle":"2023-11-29T04:56:24.929300Z","shell.execute_reply.started":"2023-11-29T04:56:11.805533Z","shell.execute_reply":"2023-11-29T04:56:24.928225Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Requirement already satisfied: dgl in /opt/conda/lib/python3.10/site-packages (1.1.2)\nRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from dgl) (1.23.5)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from dgl) (1.11.2)\nRequirement already satisfied: networkx>=2.1 in /opt/conda/lib/python3.10/site-packages (from dgl) (3.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from dgl) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from dgl) (4.66.1)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from dgl) (5.9.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->dgl) (2023.7.22)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# model\nimport torch.nn as nn\nimport dgl.nn\nfrom dgl.nn import GATConv, SGConv, SAGEConv\n\n\nclass GAT(nn.Module):\n    def __init__(self, in_feats, hidden_size, num_classes, num_layers, dropout_rate, activation):\n        super(GAT, self).__init__()\n        self.layers = nn.ModuleList()\n        self.layers.append(GATConv(in_feats, hidden_size, num_heads=1, activation=activation, allow_zero_in_degree=True))\n        for _ in range(num_layers - 2):\n            self.layers.append(GATConv(hidden_size, hidden_size, num_heads=1, activation=activation, allow_zero_in_degree=True))\n        self.layers.append(GATConv(hidden_size, num_classes, num_heads=1, activation=None, allow_zero_in_degree=True))\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, g, features):\n        h = features\n        for layer in self.layers[:-1]:\n            h = layer(g, h).flatten(1)\n            h = self.dropout(h)\n        h = self.layers[-1](g, h).mean(1)  # No activation on the final layer\n        return h\n\n\nclass GCN(nn.Module):\n    def __init__(self, in_feats, hidden_size, num_classes, activation, num_layers, dropout_rate):\n        super(GCN, self).__init__()\n        layers = [\n            dgl.nn.GraphConv(in_feats, hidden_size, activation=activation, allow_zero_in_degree=True)\n        ]\n        for _ in range(num_layers - 2):\n            layers.append(dgl.nn.GraphConv(hidden_size, hidden_size, activation=activation, allow_zero_in_degree=True))\n        layers.append(dgl.nn.GraphConv(hidden_size, num_classes, allow_zero_in_degree=True))\n        self.layers = nn.ModuleList(layers)\n        self.dropout = nn.Dropout(dropout_rate)\n\n    def forward(self, g, features):\n        h = features\n        for layer in self.layers:\n            h = layer(g, h)\n            h = self.dropout(h)\n        return h\n\n# Add Simplifying Graph Convolutional Networks (SGC)\nclass SGC(nn.Module):\n    def __init__(self, in_feats, num_classes, num_layers, activation, out_feats=256):  # Set an appropriate value for out_feats\n        super(SGC, self).__init__()\n        self.layers = nn.ModuleList()\n        for _ in range(num_layers):\n            self.layers.append(SGConv(in_feats, out_feats, allow_zero_in_degree=True))  # Use out_feats instead of num_classes\n            in_feats = out_feats  # Update in_feats for the next layer\n        self.activation = activation  # Store the activation function\n\n    def forward(self, g, features):\n        h = features\n        for layer in self.layers:\n            h = layer(g, h)\n            h = self.activation(h)  # Apply the activation function\n        return h\n\nclass GraphSAGE(nn.Module):\n    def __init__(self, in_feats, num_classes, num_layers, activation, out_feats=128):\n        super(GraphSAGE, self).__init__()\n        self.layers = nn.ModuleList()\n        self.layers.append(SAGEConv(in_feats, out_feats, 'mean'))  # Use 'mean' aggregation for the first layer\n        for _ in range(num_layers - 2):\n            self.layers.append(SAGEConv(out_feats, out_feats, 'mean'))\n        self.layers.append(SAGEConv(out_feats, num_classes, 'mean'))\n        self.activation = activation\n\n    def forward(self, g, features):\n        h = features\n        for layer in self.layers:\n            h = layer(g, h)\n            h = self.activation(h)\n        return h","metadata":{"execution":{"iopub.status.busy":"2023-11-29T04:56:27.897764Z","iopub.execute_input":"2023-11-29T04:56:27.898279Z","iopub.status.idle":"2023-11-29T04:56:27.919332Z","shell.execute_reply.started":"2023-11-29T04:56:27.898237Z","shell.execute_reply":"2023-11-29T04:56:27.918339Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import dgl\nimport torch\nimport dgl.data\nimport os\nimport numpy as np\n\nfrom tensorflow import keras\n\n# Define functions to calculate precision, recall, and F-measure\ndef calculate_metrics(pred, labels):\n    tp = (pred * labels).sum().float()\n    fp = ((pred == 1) * (labels == 0)).sum().float()\n    fn = ((pred == 0) * (labels == 1)).sum().float()\n\n    micro_precision = tp / (tp + fp + 1e-10)\n    micro_recall = tp / (tp + fn + 1e-10)\n\n    unique_labels = labels.unique()\n    macro_precision = 0.0\n    macro_recall = 0.0\n\n    for label in unique_labels:\n        tp_i = ((pred == label) * (labels == label)).sum().float()\n        fp_i = ((pred == label) * (labels != label)).sum().float()\n        fn_i = ((pred != label) * (labels == label)).sum().float()\n\n        precision_i = tp_i / (tp_i + fp_i + 1e-10)\n        recall_i = tp_i / (tp_i + fn_i + 1e-10)\n\n        macro_precision += precision_i\n        macro_recall += recall_i\n\n    macro_precision /= len(unique_labels)\n    macro_recall /= len(unique_labels)\n\n    f_measure = 2 * (macro_precision * macro_recall) / (macro_precision + macro_recall + 1e-10)\n\n    return macro_precision.item(), micro_precision.item(), macro_recall.item(), micro_recall.item(), f_measure.item()\n\n# Function to load the MNIST dataset\ndef load_mnist_dataset():\n    (train_data, train_labels), (test_data, test_labels) = keras.datasets.mnist.load_data()\n    num_train = len(train_data)\n    num_test = len(test_data)\n    num_nodes = num_train + num_test\n\n    g = dgl.DGLGraph()\n    g.add_nodes(num_nodes)\n    mnist_data = torch.cat([torch.Tensor(train_data).view(num_train, -1), torch.Tensor(test_data).view(num_test, -1)], dim=0)\n    g.ndata['feat'] = mnist_data\n    g.add_edges(range(num_nodes), range(num_nodes))\n    \n    labels = torch.cat([torch.Tensor(train_labels), torch.Tensor(test_labels)], dim=0)\n    labels = labels.long() \n    g.ndata['label'] = labels\n\n    # Create mask features\n    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n\n    train_mask[:num_train] = True\n    val_mask[num_train:num_train+1000] = True  # Assuming 1000 validation samples\n    test_mask[num_train+1000:] = True  # Assuming the rest are test samples\n\n    g.ndata['train_mask'] = train_mask\n    g.ndata['val_mask'] = val_mask\n    g.ndata['test_mask'] = test_mask\n    g.num_classes = 10\n    return g\n\ndef load_nell():\n    data = torch.load('data/nell/processed/data.pt')\n    data = data[0]\n    num_nodes = data.num_nodes\n    edge_index = data.edge_index\n    # Convert 'x' to a dense tensor\n    x = data.x\n    if x is not None:\n        x = x.to_dense()\n\n    y = data.y  # Node labels\n    train_mask = data.train_mask\n    val_mask = data.val_mask\n    test_mask = data.test_mask\n\n    # Create an empty DGL graph\n    g = dgl.DGLGraph()\n    \n    # Add nodes to the graph and explicitly set the 'num_nodes' attribute\n    g.add_nodes(num_nodes)\n\n    # Add edges to the graph\n    g.add_edges(edge_index[0], edge_index[1])\n\n    # If you have node features, you can set them for the DGL graph\n    if x is not None:\n        g.ndata['feat'] = x\n    # Set node labels\n    if y is not None:\n        g.ndata['label'] = y\n\n    # Set masks for training, validation, and testing\n    if train_mask is not None:\n        g.ndata['train_mask'] = train_mask\n    if val_mask is not None:\n        g.ndata['val_mask'] = val_mask\n    if test_mask is not None:\n        g.ndata['test_mask'] = test_mask\n    g.num_classes = 186\n    return g\n\n# Function to load datasets\ndef load_dataset(dataset_name):\n    if dataset_name in ['cora', 'citeseer', 'pubmed']:\n        dataset = dgl.data.__dict__[f'{dataset_name.capitalize()}GraphDataset']()\n    elif dataset_name == 'mnist':\n        dataset = load_mnist_dataset()\n    elif dataset_name == 'reddit':\n        dataset = dgl.data.RedditDataset()\n    elif dataset_name == 'nell':\n        dataset = load_nell()\n    elif dataset_name == 'blogcatalog':\n        dataset = load_blogcatalog()\n    return dataset\n\ndef load_blogcatalog():\n    # Load the dataset\n    dataset = dgl.data.CSVDataset('data/blogcatalog_dataset')\n\n    # Extract the graph (assuming there's only one graph in the dataset)\n    g = dataset[0]\n\n    # Generate random features for the nodes\n    num_nodes = g.number_of_nodes()\n    feat_dim = 64\n    feat = torch.randn(num_nodes, feat_dim)\n    g.ndata['feat'] = feat\n\n    # Define the number of classes (assuming a specific number, modify as needed)\n    num_classes = 39\n\n    # Generate random labels for the nodes\n    labels = torch.randint(0, num_classes, (num_nodes,))\n    g.ndata['label'] = labels\n\n    # Create masks for train, validation, and test sets\n    train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n    val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n    test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n\n    # Assuming 60% train, 20% validation, and 20% test split\n    train_mask[:int(0.6 * num_nodes)] = 1\n    val_mask[int(0.6 * num_nodes):int(0.8 * num_nodes)] = 1\n    test_mask[int(0.8 * num_nodes):] = 1\n\n    # Assign masks to the graph\n    g.ndata['train_mask'] = train_mask\n    g.ndata['val_mask'] = val_mask\n    g.ndata['test_mask'] = test_mask\n\n    # Retrieve the number of classes\n    num_classes = len(torch.unique(g.ndata['label']))\n    g.num_classes = num_classes\n    # print(\"Number of classes:\", num_classes)\n    return g","metadata":{"execution":{"iopub.status.busy":"2023-11-29T04:56:28.785887Z","iopub.execute_input":"2023-11-29T04:56:28.786371Z","iopub.status.idle":"2023-11-29T04:56:28.815260Z","shell.execute_reply.started":"2023-11-29T04:56:28.786332Z","shell.execute_reply":"2023-11-29T04:56:28.813583Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# activation function\nimport torch\nimport torch.nn.functional as F\n\n# ReLU\ndef relu(x):\n    return F.relu(x)\n\n# Leaky ReLU\ndef leaky_relu(x, alpha=0.01):\n    return F.leaky_relu(x, negative_slope=alpha)\n\n# Tanh\ndef tanh(x):\n    return torch.tanh(x)\n\n# ELU\ndef elu(x, alpha=1.0):\n    return F.elu(x, alpha=alpha)\n\n# Swish\ndef swish(x):\n    return x * torch.sigmoid(x)\n\n# Mish\ndef mish(x):\n    return x * torch.tanh(F.softplus(x))\n\n# Smish\ndef smish(x):\n    return x * torch.sigmoid(1.702 * x)\n\n# LiSHT\ndef lisht(x):\n    return x * torch.tanh(x)\n\n# HardSeReLU\ndef hard_selerelu(x):\n    return torch.where(x < 0, torch.mul(0.2, torch.expm1(x)), torch.mul(0.2 * F.relu(x), torch.clamp((x + 1) / 2, min=0, max=1)))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T04:56:29.761012Z","iopub.execute_input":"2023-11-29T04:56:29.761473Z","iopub.status.idle":"2023-11-29T04:56:29.771139Z","shell.execute_reply.started":"2023-11-29T04:56:29.761435Z","shell.execute_reply":"2023-11-29T04:56:29.770035Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T03:52:04.106745Z","iopub.execute_input":"2023-11-29T03:52:04.108324Z","iopub.status.idle":"2023-11-29T03:52:04.126412Z","shell.execute_reply.started":"2023-11-29T03:52:04.108272Z","shell.execute_reply":"2023-11-29T03:52:04.124917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import dgl\nimport torch\nimport torch.nn.functional as F\nimport os\nimport pandas as pd\nimport time\n\n# from gnn.model import GCN, GAT, GraphSAGE, SGC\n# from utils.tools import calculate_metrics, load_dataset\n# import gnn.activation_function as af\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Check if GPU is available, otherwise use CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# dataset_names = ['pubmed', 'cora', 'citeseer', 'blogcatalog', 'reddit', 'mnist', 'nell']\ndataset_names = ['reddit']\n\n# gnn_models = [GCN, GAT, GraphSAGE, SGC]\ngnn_models = [GAT]\n\n# activation_functions = [af.relu, af.tanh, af.leaky_relu, af.elu, af.swish, af.mish, af.lisht, af.smish, af.hard_selerelu]\nactivation_functions = [tanh]\n\n# num_epochs_list = [10, 50, 100, 250, 500]\nnum_epochs_list = [10]\n\n# learning_rates = [0.01, 0.00001]\nlearning_rates = [0.01]\n\nnum_layers_list = [2, 4]\n# num_layers_list = [2]\n\n# dropout_rates = [0.05]\ndropout_rates = [0.1, 0.05]\n\nhidden_sizes = [16, 32]\n# hidden_sizes = [16]\n\nresults_df = pd.DataFrame(columns=['Dataset', 'Model', 'Activation', 'Epoch', 'Loss', 'Val Acc', 'Best Val Acc', 'Test Acc', 'Best Test Acc', 'Macro Precision', 'Micro Precision', 'Macro Recall', 'Micro Recall', 'F-measure', 'Computation Time (seconds)', 'Num Epochs', 'Learning Rate', 'Num Layers', 'Dropout Rate', 'Hidden Size'])\n\n# Define a function to train the model and save train accuracy\ndef train(g, model, dataset_name, gnn_name, activation_name, num_epochs, learning_rate, num_layers, dropout_rate, hidden_size, run_number):\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n    best_val_acc = 0\n    best_test_acc = 0\n    best_train_acc = 0\n\n    features = g.ndata['feat'].to(device)\n    labels = g.ndata['label'].to(device)\n    train_mask = g.ndata['train_mask'].to(device)\n    val_mask = g.ndata['val_mask'].to(device)\n    test_mask = g.ndata['test_mask'].to(device)\n\n    g = dgl.add_self_loop(g)\n    results_dfs = []\n\n    for epoch in range(num_epochs):\n        start_time = time.time()\n\n        logits = model(g.to(device), features)\n        pred = logits.argmax(1)\n        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n\n        macro_precision, micro_precision, macro_recall, micro_recall, f_measure = calculate_metrics(pred[test_mask], labels[test_mask])\n\n        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n\n        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n\n        if best_val_acc < val_acc:\n            best_val_acc = val_acc\n\n        if best_test_acc < test_acc:\n            best_test_acc = test_acc\n\n        if best_train_acc < train_acc:\n            best_train_acc = train_acc    \n\n        if isinstance(best_test_acc, int):\n            best_test_acc_value = best_test_acc\n        else:\n            best_test_acc_value = best_test_acc.item()\n\n        if isinstance(best_val_acc, int):\n            best_val_acc_value = best_val_acc\n        else:\n            best_val_acc_value = best_val_acc.item()\n\n        if isinstance(best_train_acc, int):\n            best_train_acc_value = best_train_acc\n        else:\n            best_train_acc_value = best_train_acc.item()    \n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        end_time = time.time()\n        computation_time = end_time - start_time\n\n        epoch_result = pd.DataFrame({\n            'Run Number': [run_number],\n            'Dataset': [dataset_name],\n            'Activation Function': [activation_name],\n            'GNN Model': [gnn_name],\n            'Learning Rate': [learning_rate],\n            'Num Layers': [num_layers],\n            'Dropout Rate': [dropout_rate],\n            'Hidden Size': [hidden_size],\n            'Num Epochs': [num_epochs],\n            'Epochs': [epoch + 1],\n            'Loss': [loss.item()],\n            'Train Acc': [train_acc.item()],\n            'Best Train Acc': [best_train_acc_value],\n            'Valid Acc': [val_acc.item()],\n            'Best Valid Acc': [best_val_acc_value],\n            'Test Acc': [test_acc.item()],\n            'Best Test Acc': [best_test_acc_value],\n            'Macro Precision': [macro_precision],\n            'Micro Precision': [micro_precision],\n            'Macro Recall': [macro_recall],\n            'Micro Recall': [micro_recall],\n            'F-measure': [f_measure],\n            'Computation Time (seconds)': [computation_time]\n        })\n        results_dfs.append(epoch_result)\n\n    print(\"Train Done\")\n    results_df = pd.concat(results_dfs, ignore_index=True)\n    return results_df\n\n# Function to create folders if they don't exist\ndef create_folders(*folders):\n    for folder in folders:\n        if not os.path.exists(folder):\n            os.makedirs(folder)\n\n# Function to save basic results to an Excel file with a formatted filename\ndef save_results_to_excel(data_name, gnn_name, activation_name, results_df):\n    # Create folder structure\n    result_folder = 'result_node_classification50'\n    dataset_folder = os.path.join(result_folder, data_name)\n    gnn_folder = os.path.join(dataset_folder, gnn_name)\n    activation_folder = os.path.join(gnn_folder, activation_name)\n    \n    create_folders(result_folder, dataset_folder, gnn_folder, activation_folder)\n    \n    # Construct the file name\n    file_name = f'{data_name}_{gnn_name}_{activation_name}.xlsx'\n\n    # Save the DataFrame to the Excel file with the constructed file path\n    file_path = os.path.join(activation_folder, file_name)\n    results_df.to_excel(file_path, index=False)\n\n# Function to save detailed results to an Excel file with a formatted filename\ndef save_detailed_results_to_excel(data_name, gnn_name, activation_name, num_epochs, learning_rate, num_layers, dropout_rate, hidden_size, results_df):\n    # Create folder structure\n    result_folder = 'result_node_classification50'\n    dataset_folder = os.path.join(result_folder, data_name)\n    gnn_folder = os.path.join(dataset_folder, gnn_name)\n    activation_folder = os.path.join(gnn_folder, activation_name)\n    \n    create_folders(result_folder, dataset_folder, gnn_folder, activation_folder)\n    \n    # Construct the file name for detailed results\n    file_name = f'{data_name}_{gnn_name}_{activation_name}_epochs{num_epochs}_lr{learning_rate}_layers{num_layers}_dropout{dropout_rate}_hidden{hidden_size}_detailed_results.xlsx'\n\n    # Save the DataFrame to the Excel file with the constructed file path\n    file_path = os.path.join(activation_folder, file_name)\n    results_df.to_excel(file_path, index=False)\n\nresults = []\nrun_number = 1\nfor data_name in dataset_names:\n    dataset = load_dataset(data_name)\n    for gnn in gnn_models:\n        for activation in activation_functions:\n            results_data_gnn_activation = []\n            for num_epochs in num_epochs_list:\n                for learning_rate in learning_rates:\n                    for num_layers in num_layers_list:\n                        for dropout_rate in dropout_rates:\n                            for hidden_size in hidden_sizes:\n                                print(f'Run number: {run_number}\\tDataset Name: {data_name}\\tModel GNN: {gnn.__name__}\\tActivation function: {activation.__name__}\\tNum Epochs: {num_epochs}\\tLearning Rate: {learning_rate}\\tNum Layers: {num_layers}\\tDropout Rate: {dropout_rate}\\tHidden Size: {hidden_size}')\n                                if data_name == 'ppi':\n                                    dataset.num_classes = dataset.num_labels\n                                elif data_name == 'mnist':\n                                    g = dataset\n                                    num_classes = dataset.num_classes\n                                    out_feats = 128\n                                elif data_name == 'nell':\n                                    g = dataset\n                                    num_classes = dataset.num_classes\n                                    out_feats = num_classes\n                                elif data_name == 'blogcatalog':\n                                    g = dataset\n                                    num_classes = g.num_classes\n                                else:\n                                    num_classes = dataset.num_classes\n                                    out_feats = 128\n                                    g = dataset[0]\n\n#                                 print('Number of categories:', num_classes)\n\n                                if (gnn == GraphSAGE) or (gnn == SGC):\n                                    model = gnn(g.ndata['feat'].shape[1], num_classes, num_layers, activation, out_feats).to(device)\n                                else:\n                                    model = gnn(g.ndata['feat'].shape[1], hidden_size=hidden_size, num_classes=num_classes, num_layers=num_layers, dropout_rate=dropout_rate, activation=activation).to(device)\n                                start_time1 = time.time()\n                                print(start_time1)\n\n                                result = train(g, model, data_name, gnn.__name__, activation.__name__, num_epochs, learning_rate, num_layers, dropout_rate, hidden_size, run_number)\n                                end_time1 = time.time()\n                                print(end_time1)\n                                computation_time1 = end_time1 - start_time1\n                                print(computation_time1)\n                                train_df = pd.concat([result], ignore_index=True)\n                                save_detailed_results_to_excel(data_name, gnn.__name__, activation.__name__, num_epochs, learning_rate, num_layers, dropout_rate, hidden_size, train_df)\n                                results_data_gnn_activation.append(result)\n                                results.append(result)\n                                run_number=run_number+1\n            results_data_gnn_activation_df = pd.concat(results_data_gnn_activation, ignore_index=True)                    \n            save_results_to_excel(data_name, gnn.__name__, activation.__name__, results_data_gnn_activation_df)\n            \nresults_df = pd.concat(results, ignore_index=True)\n\nresults_df.to_excel(f'result_node_classification50/results_node_classification_{dataset_names}.xlsx', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T04:57:50.019668Z","iopub.execute_input":"2023-11-29T04:57:50.022453Z","iopub.status.idle":"2023-11-29T07:38:22.727929Z","shell.execute_reply.started":"2023-11-29T04:57:50.022085Z","shell.execute_reply":"2023-11-29T07:38:22.723829Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Run number: 1\tDataset Name: reddit\tModel GNN: GAT\tActivation function: tanh\tNum Epochs: 10\tLearning Rate: 0.01\tNum Layers: 2\tDropout Rate: 0.1\tHidden Size: 16\n1701233892.4068606\nTrain Done\n1701234698.6821005\n806.275239944458\nRun number: 2\tDataset Name: reddit\tModel GNN: GAT\tActivation function: tanh\tNum Epochs: 10\tLearning Rate: 0.01\tNum Layers: 2\tDropout Rate: 0.1\tHidden Size: 32\n1701234698.709915\nTrain Done\n1701235597.2422466\n898.5323317050934\nRun number: 3\tDataset Name: reddit\tModel GNN: GAT\tActivation function: tanh\tNum Epochs: 10\tLearning Rate: 0.01\tNum Layers: 2\tDropout Rate: 0.05\tHidden Size: 16\n1701235597.265659\nTrain Done\n1701236358.5539277\n761.2882685661316\nRun number: 4\tDataset Name: reddit\tModel GNN: GAT\tActivation function: tanh\tNum Epochs: 10\tLearning Rate: 0.01\tNum Layers: 2\tDropout Rate: 0.05\tHidden Size: 32\n1701236358.5795617\nTrain Done\n1701237230.685063\n872.1055011749268\nRun number: 5\tDataset Name: reddit\tModel GNN: GAT\tActivation function: tanh\tNum Epochs: 10\tLearning Rate: 0.01\tNum Layers: 4\tDropout Rate: 0.1\tHidden Size: 16\n1701237230.707847\nTrain Done\n1701238545.258265\n1314.5504179000854\nRun number: 6\tDataset Name: reddit\tModel GNN: GAT\tActivation function: tanh\tNum Epochs: 10\tLearning Rate: 0.01\tNum Layers: 4\tDropout Rate: 0.1\tHidden Size: 32\n1701238545.2845662\nTrain Done\n1701240227.6194005\n1682.3348343372345\nRun number: 7\tDataset Name: reddit\tModel GNN: GAT\tActivation function: tanh\tNum Epochs: 10\tLearning Rate: 0.01\tNum Layers: 4\tDropout Rate: 0.05\tHidden Size: 16\n1701240227.661605\nTrain Done\n1701241581.4633844\n1353.8017795085907\nRun number: 8\tDataset Name: reddit\tModel GNN: GAT\tActivation function: tanh\tNum Epochs: 10\tLearning Rate: 0.01\tNum Layers: 4\tDropout Rate: 0.05\tHidden Size: 32\n1701241581.4890804\nTrain Done\n1701243502.5652275\n1921.0761470794678\n","output_type":"stream"}]},{"cell_type":"code","source":"results_df","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:38:22.737602Z","iopub.execute_input":"2023-11-29T07:38:22.738310Z","iopub.status.idle":"2023-11-29T07:38:22.798406Z","shell.execute_reply.started":"2023-11-29T07:38:22.738199Z","shell.execute_reply":"2023-11-29T07:38:22.797198Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"    Run Number Dataset Activation Function GNN Model  Learning Rate  \\\n0            1  reddit                tanh       GAT           0.01   \n1            1  reddit                tanh       GAT           0.01   \n2            1  reddit                tanh       GAT           0.01   \n3            1  reddit                tanh       GAT           0.01   \n4            1  reddit                tanh       GAT           0.01   \n..         ...     ...                 ...       ...            ...   \n75           8  reddit                tanh       GAT           0.01   \n76           8  reddit                tanh       GAT           0.01   \n77           8  reddit                tanh       GAT           0.01   \n78           8  reddit                tanh       GAT           0.01   \n79           8  reddit                tanh       GAT           0.01   \n\n    Num Layers  Dropout Rate  Hidden Size  Num Epochs  Epochs  ...  Valid Acc  \\\n0            2          0.10           16          10       1  ...   0.063657   \n1            2          0.10           16          10       2  ...   0.186648   \n2            2          0.10           16          10       3  ...   0.208384   \n3            2          0.10           16          10       4  ...   0.281860   \n4            2          0.10           16          10       5  ...   0.284294   \n..         ...           ...          ...         ...     ...  ...        ...   \n75           4          0.05           32          10       6  ...   0.398263   \n76           4          0.05           32          10       7  ...   0.401704   \n77           4          0.05           32          10       8  ...   0.416558   \n78           4          0.05           32          10       9  ...   0.419244   \n79           4          0.05           32          10      10  ...   0.453905   \n\n    Best Valid Acc  Test Acc  Best Test Acc  Macro Precision  Micro Precision  \\\n0         0.063657  0.063049       0.063049         0.031877         0.999999   \n1         0.186648  0.185179       0.185179         0.054037         1.000000   \n2         0.208384  0.207493       0.207493         0.106770         1.000000   \n3         0.281860  0.278369       0.278369         0.127036         1.000000   \n4         0.284294  0.279410       0.279410         0.154734         1.000000   \n..             ...       ...            ...              ...              ...   \n75        0.398263  0.398776       0.398776         0.132766         1.000000   \n76        0.401704  0.403748       0.403748         0.139734         1.000000   \n77        0.416558  0.418092       0.418092         0.139485         1.000000   \n78        0.419244  0.421683       0.421683         0.159475         1.000000   \n79        0.453905  0.456852       0.456852         0.180155         1.000000   \n\n    Macro Recall  Micro Recall  F-measure  Computation Time (seconds)  \n0       0.033581           1.0   0.032707                   90.963950  \n1       0.061255           1.0   0.057420                   80.575576  \n2       0.070873           1.0   0.085195                   79.265591  \n3       0.108272           1.0   0.116906                   76.350540  \n4       0.121825           1.0   0.136322                   75.722263  \n..           ...           ...        ...                         ...  \n75      0.130837           1.0   0.131795                  187.085221  \n76      0.132241           1.0   0.135884                  198.752966  \n77      0.151134           1.0   0.145076                  200.294433  \n78      0.157293           1.0   0.158376                  205.763353  \n79      0.188096           1.0   0.184040                  205.498436  \n\n[80 rows x 23 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Run Number</th>\n      <th>Dataset</th>\n      <th>Activation Function</th>\n      <th>GNN Model</th>\n      <th>Learning Rate</th>\n      <th>Num Layers</th>\n      <th>Dropout Rate</th>\n      <th>Hidden Size</th>\n      <th>Num Epochs</th>\n      <th>Epochs</th>\n      <th>...</th>\n      <th>Valid Acc</th>\n      <th>Best Valid Acc</th>\n      <th>Test Acc</th>\n      <th>Best Test Acc</th>\n      <th>Macro Precision</th>\n      <th>Micro Precision</th>\n      <th>Macro Recall</th>\n      <th>Micro Recall</th>\n      <th>F-measure</th>\n      <th>Computation Time (seconds)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>reddit</td>\n      <td>tanh</td>\n      <td>GAT</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>0.10</td>\n      <td>16</td>\n      <td>10</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0.063657</td>\n      <td>0.063657</td>\n      <td>0.063049</td>\n      <td>0.063049</td>\n      <td>0.031877</td>\n      <td>0.999999</td>\n      <td>0.033581</td>\n      <td>1.0</td>\n      <td>0.032707</td>\n      <td>90.963950</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>reddit</td>\n      <td>tanh</td>\n      <td>GAT</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>0.10</td>\n      <td>16</td>\n      <td>10</td>\n      <td>2</td>\n      <td>...</td>\n      <td>0.186648</td>\n      <td>0.186648</td>\n      <td>0.185179</td>\n      <td>0.185179</td>\n      <td>0.054037</td>\n      <td>1.000000</td>\n      <td>0.061255</td>\n      <td>1.0</td>\n      <td>0.057420</td>\n      <td>80.575576</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>reddit</td>\n      <td>tanh</td>\n      <td>GAT</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>0.10</td>\n      <td>16</td>\n      <td>10</td>\n      <td>3</td>\n      <td>...</td>\n      <td>0.208384</td>\n      <td>0.208384</td>\n      <td>0.207493</td>\n      <td>0.207493</td>\n      <td>0.106770</td>\n      <td>1.000000</td>\n      <td>0.070873</td>\n      <td>1.0</td>\n      <td>0.085195</td>\n      <td>79.265591</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>reddit</td>\n      <td>tanh</td>\n      <td>GAT</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>0.10</td>\n      <td>16</td>\n      <td>10</td>\n      <td>4</td>\n      <td>...</td>\n      <td>0.281860</td>\n      <td>0.281860</td>\n      <td>0.278369</td>\n      <td>0.278369</td>\n      <td>0.127036</td>\n      <td>1.000000</td>\n      <td>0.108272</td>\n      <td>1.0</td>\n      <td>0.116906</td>\n      <td>76.350540</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>reddit</td>\n      <td>tanh</td>\n      <td>GAT</td>\n      <td>0.01</td>\n      <td>2</td>\n      <td>0.10</td>\n      <td>16</td>\n      <td>10</td>\n      <td>5</td>\n      <td>...</td>\n      <td>0.284294</td>\n      <td>0.284294</td>\n      <td>0.279410</td>\n      <td>0.279410</td>\n      <td>0.154734</td>\n      <td>1.000000</td>\n      <td>0.121825</td>\n      <td>1.0</td>\n      <td>0.136322</td>\n      <td>75.722263</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>8</td>\n      <td>reddit</td>\n      <td>tanh</td>\n      <td>GAT</td>\n      <td>0.01</td>\n      <td>4</td>\n      <td>0.05</td>\n      <td>32</td>\n      <td>10</td>\n      <td>6</td>\n      <td>...</td>\n      <td>0.398263</td>\n      <td>0.398263</td>\n      <td>0.398776</td>\n      <td>0.398776</td>\n      <td>0.132766</td>\n      <td>1.000000</td>\n      <td>0.130837</td>\n      <td>1.0</td>\n      <td>0.131795</td>\n      <td>187.085221</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>8</td>\n      <td>reddit</td>\n      <td>tanh</td>\n      <td>GAT</td>\n      <td>0.01</td>\n      <td>4</td>\n      <td>0.05</td>\n      <td>32</td>\n      <td>10</td>\n      <td>7</td>\n      <td>...</td>\n      <td>0.401704</td>\n      <td>0.401704</td>\n      <td>0.403748</td>\n      <td>0.403748</td>\n      <td>0.139734</td>\n      <td>1.000000</td>\n      <td>0.132241</td>\n      <td>1.0</td>\n      <td>0.135884</td>\n      <td>198.752966</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>8</td>\n      <td>reddit</td>\n      <td>tanh</td>\n      <td>GAT</td>\n      <td>0.01</td>\n      <td>4</td>\n      <td>0.05</td>\n      <td>32</td>\n      <td>10</td>\n      <td>8</td>\n      <td>...</td>\n      <td>0.416558</td>\n      <td>0.416558</td>\n      <td>0.418092</td>\n      <td>0.418092</td>\n      <td>0.139485</td>\n      <td>1.000000</td>\n      <td>0.151134</td>\n      <td>1.0</td>\n      <td>0.145076</td>\n      <td>200.294433</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>8</td>\n      <td>reddit</td>\n      <td>tanh</td>\n      <td>GAT</td>\n      <td>0.01</td>\n      <td>4</td>\n      <td>0.05</td>\n      <td>32</td>\n      <td>10</td>\n      <td>9</td>\n      <td>...</td>\n      <td>0.419244</td>\n      <td>0.419244</td>\n      <td>0.421683</td>\n      <td>0.421683</td>\n      <td>0.159475</td>\n      <td>1.000000</td>\n      <td>0.157293</td>\n      <td>1.0</td>\n      <td>0.158376</td>\n      <td>205.763353</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>8</td>\n      <td>reddit</td>\n      <td>tanh</td>\n      <td>GAT</td>\n      <td>0.01</td>\n      <td>4</td>\n      <td>0.05</td>\n      <td>32</td>\n      <td>10</td>\n      <td>10</td>\n      <td>...</td>\n      <td>0.453905</td>\n      <td>0.453905</td>\n      <td>0.456852</td>\n      <td>0.456852</td>\n      <td>0.180155</td>\n      <td>1.000000</td>\n      <td>0.188096</td>\n      <td>1.0</td>\n      <td>0.184040</td>\n      <td>205.498436</td>\n    </tr>\n  </tbody>\n</table>\n<p>80 rows Ã— 23 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:40:42.081351Z","iopub.execute_input":"2023-11-29T07:40:42.082339Z","iopub.status.idle":"2023-11-29T07:40:43.200844Z","shell.execute_reply.started":"2023-11-29T07:40:42.082295Z","shell.execute_reply":"2023-11-29T07:40:43.199417Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"redit_gat_tanh.zip  redit_sgc_tanh.zip\tresult_node_classification50\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip -r redit_gat_tanh.zip /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:38:25.094104Z","iopub.status.idle":"2023-11-29T07:38:25.094511Z","shell.execute_reply.started":"2023-11-29T07:38:25.094290Z","shell.execute_reply":"2023-11-29T07:38:25.094306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:38:23.947149Z","iopub.execute_input":"2023-11-29T07:38:23.947613Z","iopub.status.idle":"2023-11-29T07:38:25.049296Z","shell.execute_reply.started":"2023-11-29T07:38:23.947569Z","shell.execute_reply":"2023-11-29T07:38:25.047774Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"redit_gat_tanh.zip  redit_sgc_tanh.zip\tresult_node_classification50\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'redit_gat_tanh.zip')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T07:41:14.364235Z","iopub.execute_input":"2023-11-29T07:41:14.365104Z","iopub.status.idle":"2023-11-29T07:41:14.373205Z","shell.execute_reply.started":"2023-11-29T07:41:14.365057Z","shell.execute_reply":"2023-11-29T07:41:14.372010Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/redit_gat_tanh.zip","text/html":"<a href='redit_gat_tanh.zip' target='_blank'>redit_gat_tanh.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}